{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCqIIVMB+sh4AUJza8qQQd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **üìå Binning (Diskritisasi) Menggunakan K-Means Clustering**\n","üéØ Tujuan\n","Diskritisasi (binning) adalah proses mengubah data numerik kontinu menjadi bentuk kategorikal (diskrit). Teknik ini sering digunakan untuk:\n","\n","Menyederhanakan model klasifikasi\n","\n","Mengurangi noise dalam data\n","\n","Membantu interpretasi data\n","\n","Salah satu pendekatan diskritisasi yang lebih fleksibel dibanding fixed-width binning adalah menggunakan K-Means Clustering.\n","\n","# **ü§ñ Apa Itu K-Means Clustering untuk Diskritisasi?**\n","Alih-alih membuat interval berdasarkan rentang nilai (misalnya ‚Äúlow‚Äù, ‚Äúmedium‚Äù, ‚Äúhigh‚Äù), K-Means digunakan untuk mengelompokkan nilai-nilai numerik menjadi klaster, berdasarkan kesamaan nilai. Setiap klaster akan dianggap sebagai satu bin."],"metadata":{"id":"T_PtjQ5fnmTm"}},{"cell_type":"markdown","source":["## Import library yang diperlukan"],"metadata":{"id":"sjy01UEHn6d9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMG02Udr4TAy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import KBinsDiscretizer\n","from sklearn.cluster import KMeans\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from tabulate import tabulate"]},{"cell_type":"markdown","source":["## Memuat Data Iris"],"metadata":{"id":"bzKnpRvZoEXc"}},{"cell_type":"code","source":["# Load dataset\n","from sklearn.datasets import load_iris\n","\n","iris = load_iris()\n","X = pd.DataFrame(iris.data, columns=iris.feature_names)\n","y = iris.target\n","\n","# display(X)\n","print(tabulate(X, headers='keys', tablefmt='psql'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdQ_6wtg42b-","executionInfo":{"status":"ok","timestamp":1749808486830,"user_tz":-420,"elapsed":77,"user":{"displayName":"23-112 Fariel Nur Rizky Maulana","userId":"07674155344665806922"}},"outputId":"c7de9fec-e9b4-4285-bfde-f3bc1a74fe1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---------------------+--------------------+---------------------+--------------------+\n","|     |   sepal length (cm) |   sepal width (cm) |   petal length (cm) |   petal width (cm) |\n","|-----+---------------------+--------------------+---------------------+--------------------|\n","|   0 |                 5.1 |                3.5 |                 1.4 |                0.2 |\n","|   1 |                 4.9 |                3   |                 1.4 |                0.2 |\n","|   2 |                 4.7 |                3.2 |                 1.3 |                0.2 |\n","|   3 |                 4.6 |                3.1 |                 1.5 |                0.2 |\n","|   4 |                 5   |                3.6 |                 1.4 |                0.2 |\n","|   5 |                 5.4 |                3.9 |                 1.7 |                0.4 |\n","|   6 |                 4.6 |                3.4 |                 1.4 |                0.3 |\n","|   7 |                 5   |                3.4 |                 1.5 |                0.2 |\n","|   8 |                 4.4 |                2.9 |                 1.4 |                0.2 |\n","|   9 |                 4.9 |                3.1 |                 1.5 |                0.1 |\n","|  10 |                 5.4 |                3.7 |                 1.5 |                0.2 |\n","|  11 |                 4.8 |                3.4 |                 1.6 |                0.2 |\n","|  12 |                 4.8 |                3   |                 1.4 |                0.1 |\n","|  13 |                 4.3 |                3   |                 1.1 |                0.1 |\n","|  14 |                 5.8 |                4   |                 1.2 |                0.2 |\n","|  15 |                 5.7 |                4.4 |                 1.5 |                0.4 |\n","|  16 |                 5.4 |                3.9 |                 1.3 |                0.4 |\n","|  17 |                 5.1 |                3.5 |                 1.4 |                0.3 |\n","|  18 |                 5.7 |                3.8 |                 1.7 |                0.3 |\n","|  19 |                 5.1 |                3.8 |                 1.5 |                0.3 |\n","|  20 |                 5.4 |                3.4 |                 1.7 |                0.2 |\n","|  21 |                 5.1 |                3.7 |                 1.5 |                0.4 |\n","|  22 |                 4.6 |                3.6 |                 1   |                0.2 |\n","|  23 |                 5.1 |                3.3 |                 1.7 |                0.5 |\n","|  24 |                 4.8 |                3.4 |                 1.9 |                0.2 |\n","|  25 |                 5   |                3   |                 1.6 |                0.2 |\n","|  26 |                 5   |                3.4 |                 1.6 |                0.4 |\n","|  27 |                 5.2 |                3.5 |                 1.5 |                0.2 |\n","|  28 |                 5.2 |                3.4 |                 1.4 |                0.2 |\n","|  29 |                 4.7 |                3.2 |                 1.6 |                0.2 |\n","|  30 |                 4.8 |                3.1 |                 1.6 |                0.2 |\n","|  31 |                 5.4 |                3.4 |                 1.5 |                0.4 |\n","|  32 |                 5.2 |                4.1 |                 1.5 |                0.1 |\n","|  33 |                 5.5 |                4.2 |                 1.4 |                0.2 |\n","|  34 |                 4.9 |                3.1 |                 1.5 |                0.2 |\n","|  35 |                 5   |                3.2 |                 1.2 |                0.2 |\n","|  36 |                 5.5 |                3.5 |                 1.3 |                0.2 |\n","|  37 |                 4.9 |                3.6 |                 1.4 |                0.1 |\n","|  38 |                 4.4 |                3   |                 1.3 |                0.2 |\n","|  39 |                 5.1 |                3.4 |                 1.5 |                0.2 |\n","|  40 |                 5   |                3.5 |                 1.3 |                0.3 |\n","|  41 |                 4.5 |                2.3 |                 1.3 |                0.3 |\n","|  42 |                 4.4 |                3.2 |                 1.3 |                0.2 |\n","|  43 |                 5   |                3.5 |                 1.6 |                0.6 |\n","|  44 |                 5.1 |                3.8 |                 1.9 |                0.4 |\n","|  45 |                 4.8 |                3   |                 1.4 |                0.3 |\n","|  46 |                 5.1 |                3.8 |                 1.6 |                0.2 |\n","|  47 |                 4.6 |                3.2 |                 1.4 |                0.2 |\n","|  48 |                 5.3 |                3.7 |                 1.5 |                0.2 |\n","|  49 |                 5   |                3.3 |                 1.4 |                0.2 |\n","|  50 |                 7   |                3.2 |                 4.7 |                1.4 |\n","|  51 |                 6.4 |                3.2 |                 4.5 |                1.5 |\n","|  52 |                 6.9 |                3.1 |                 4.9 |                1.5 |\n","|  53 |                 5.5 |                2.3 |                 4   |                1.3 |\n","|  54 |                 6.5 |                2.8 |                 4.6 |                1.5 |\n","|  55 |                 5.7 |                2.8 |                 4.5 |                1.3 |\n","|  56 |                 6.3 |                3.3 |                 4.7 |                1.6 |\n","|  57 |                 4.9 |                2.4 |                 3.3 |                1   |\n","|  58 |                 6.6 |                2.9 |                 4.6 |                1.3 |\n","|  59 |                 5.2 |                2.7 |                 3.9 |                1.4 |\n","|  60 |                 5   |                2   |                 3.5 |                1   |\n","|  61 |                 5.9 |                3   |                 4.2 |                1.5 |\n","|  62 |                 6   |                2.2 |                 4   |                1   |\n","|  63 |                 6.1 |                2.9 |                 4.7 |                1.4 |\n","|  64 |                 5.6 |                2.9 |                 3.6 |                1.3 |\n","|  65 |                 6.7 |                3.1 |                 4.4 |                1.4 |\n","|  66 |                 5.6 |                3   |                 4.5 |                1.5 |\n","|  67 |                 5.8 |                2.7 |                 4.1 |                1   |\n","|  68 |                 6.2 |                2.2 |                 4.5 |                1.5 |\n","|  69 |                 5.6 |                2.5 |                 3.9 |                1.1 |\n","|  70 |                 5.9 |                3.2 |                 4.8 |                1.8 |\n","|  71 |                 6.1 |                2.8 |                 4   |                1.3 |\n","|  72 |                 6.3 |                2.5 |                 4.9 |                1.5 |\n","|  73 |                 6.1 |                2.8 |                 4.7 |                1.2 |\n","|  74 |                 6.4 |                2.9 |                 4.3 |                1.3 |\n","|  75 |                 6.6 |                3   |                 4.4 |                1.4 |\n","|  76 |                 6.8 |                2.8 |                 4.8 |                1.4 |\n","|  77 |                 6.7 |                3   |                 5   |                1.7 |\n","|  78 |                 6   |                2.9 |                 4.5 |                1.5 |\n","|  79 |                 5.7 |                2.6 |                 3.5 |                1   |\n","|  80 |                 5.5 |                2.4 |                 3.8 |                1.1 |\n","|  81 |                 5.5 |                2.4 |                 3.7 |                1   |\n","|  82 |                 5.8 |                2.7 |                 3.9 |                1.2 |\n","|  83 |                 6   |                2.7 |                 5.1 |                1.6 |\n","|  84 |                 5.4 |                3   |                 4.5 |                1.5 |\n","|  85 |                 6   |                3.4 |                 4.5 |                1.6 |\n","|  86 |                 6.7 |                3.1 |                 4.7 |                1.5 |\n","|  87 |                 6.3 |                2.3 |                 4.4 |                1.3 |\n","|  88 |                 5.6 |                3   |                 4.1 |                1.3 |\n","|  89 |                 5.5 |                2.5 |                 4   |                1.3 |\n","|  90 |                 5.5 |                2.6 |                 4.4 |                1.2 |\n","|  91 |                 6.1 |                3   |                 4.6 |                1.4 |\n","|  92 |                 5.8 |                2.6 |                 4   |                1.2 |\n","|  93 |                 5   |                2.3 |                 3.3 |                1   |\n","|  94 |                 5.6 |                2.7 |                 4.2 |                1.3 |\n","|  95 |                 5.7 |                3   |                 4.2 |                1.2 |\n","|  96 |                 5.7 |                2.9 |                 4.2 |                1.3 |\n","|  97 |                 6.2 |                2.9 |                 4.3 |                1.3 |\n","|  98 |                 5.1 |                2.5 |                 3   |                1.1 |\n","|  99 |                 5.7 |                2.8 |                 4.1 |                1.3 |\n","| 100 |                 6.3 |                3.3 |                 6   |                2.5 |\n","| 101 |                 5.8 |                2.7 |                 5.1 |                1.9 |\n","| 102 |                 7.1 |                3   |                 5.9 |                2.1 |\n","| 103 |                 6.3 |                2.9 |                 5.6 |                1.8 |\n","| 104 |                 6.5 |                3   |                 5.8 |                2.2 |\n","| 105 |                 7.6 |                3   |                 6.6 |                2.1 |\n","| 106 |                 4.9 |                2.5 |                 4.5 |                1.7 |\n","| 107 |                 7.3 |                2.9 |                 6.3 |                1.8 |\n","| 108 |                 6.7 |                2.5 |                 5.8 |                1.8 |\n","| 109 |                 7.2 |                3.6 |                 6.1 |                2.5 |\n","| 110 |                 6.5 |                3.2 |                 5.1 |                2   |\n","| 111 |                 6.4 |                2.7 |                 5.3 |                1.9 |\n","| 112 |                 6.8 |                3   |                 5.5 |                2.1 |\n","| 113 |                 5.7 |                2.5 |                 5   |                2   |\n","| 114 |                 5.8 |                2.8 |                 5.1 |                2.4 |\n","| 115 |                 6.4 |                3.2 |                 5.3 |                2.3 |\n","| 116 |                 6.5 |                3   |                 5.5 |                1.8 |\n","| 117 |                 7.7 |                3.8 |                 6.7 |                2.2 |\n","| 118 |                 7.7 |                2.6 |                 6.9 |                2.3 |\n","| 119 |                 6   |                2.2 |                 5   |                1.5 |\n","| 120 |                 6.9 |                3.2 |                 5.7 |                2.3 |\n","| 121 |                 5.6 |                2.8 |                 4.9 |                2   |\n","| 122 |                 7.7 |                2.8 |                 6.7 |                2   |\n","| 123 |                 6.3 |                2.7 |                 4.9 |                1.8 |\n","| 124 |                 6.7 |                3.3 |                 5.7 |                2.1 |\n","| 125 |                 7.2 |                3.2 |                 6   |                1.8 |\n","| 126 |                 6.2 |                2.8 |                 4.8 |                1.8 |\n","| 127 |                 6.1 |                3   |                 4.9 |                1.8 |\n","| 128 |                 6.4 |                2.8 |                 5.6 |                2.1 |\n","| 129 |                 7.2 |                3   |                 5.8 |                1.6 |\n","| 130 |                 7.4 |                2.8 |                 6.1 |                1.9 |\n","| 131 |                 7.9 |                3.8 |                 6.4 |                2   |\n","| 132 |                 6.4 |                2.8 |                 5.6 |                2.2 |\n","| 133 |                 6.3 |                2.8 |                 5.1 |                1.5 |\n","| 134 |                 6.1 |                2.6 |                 5.6 |                1.4 |\n","| 135 |                 7.7 |                3   |                 6.1 |                2.3 |\n","| 136 |                 6.3 |                3.4 |                 5.6 |                2.4 |\n","| 137 |                 6.4 |                3.1 |                 5.5 |                1.8 |\n","| 138 |                 6   |                3   |                 4.8 |                1.8 |\n","| 139 |                 6.9 |                3.1 |                 5.4 |                2.1 |\n","| 140 |                 6.7 |                3.1 |                 5.6 |                2.4 |\n","| 141 |                 6.9 |                3.1 |                 5.1 |                2.3 |\n","| 142 |                 5.8 |                2.7 |                 5.1 |                1.9 |\n","| 143 |                 6.8 |                3.2 |                 5.9 |                2.3 |\n","| 144 |                 6.7 |                3.3 |                 5.7 |                2.5 |\n","| 145 |                 6.7 |                3   |                 5.2 |                2.3 |\n","| 146 |                 6.3 |                2.5 |                 5   |                1.9 |\n","| 147 |                 6.5 |                3   |                 5.2 |                2   |\n","| 148 |                 6.2 |                3.4 |                 5.4 |                2.3 |\n","| 149 |                 5.9 |                3   |                 5.1 |                1.8 |\n","+-----+---------------------+--------------------+---------------------+--------------------+\n"]}]},{"cell_type":"markdown","source":["## üìä Diskritisasi Data dengan K-Means Clustering (Binning)\n","Pada bagian ini, kita akan melakukan diskritisasi (binning) terhadap data numerik dari dataset Iris menggunakan algoritma K-Means Clustering. Diskritisasi ini bertujuan untuk mengubah nilai kontinu (float) menjadi label diskrit (kategori), yang sering dibutuhkan dalam model-model klasifikasi atau visualisasi tertentu.\n","\n","Langkah-langkah Kode:\n","\n","\n","1.   Menyalin Data Asli\n","\n","    Data awal X disalin ke variabel X_discretized agar proses modifikasi tidak mengubah dataset aslinya.\n","2.   Menentukan Jumlah Klaster per Fitur\n","\n","    Kita mendefinisikan jumlah klaster yang berbeda-beda untuk setiap fitur:\n","\n","    - sepal length (cm) ‚Üí 4 klaster\n","\n","    - sepal width (cm) ‚Üí 3 klaster\n","\n","    - petal length (cm) ‚Üí 4 klaster\n","\n","    - petal width (cm) ‚Üí 3 klaster\n","    Hal ini memungkinkan kita melakukan diskritisasi yang disesuaikan dengan karakteristik distribusi tiap fitur.\n","\n","3. Melakukan K-Means Clustering\n","Untuk setiap fitur, algoritma K-Means dijalankan secara individual:\n","\n","    - Data fitur dibentuk ulang menjadi array 2 dimensi (X[[col]]).\n","\n","    - Model KMeans dengan jumlah klaster tertentu dilatih (fit_predict).\n","\n","    - Label klaster yang dihasilkan menggantikan nilai asli pada fitur tersebut.\n","\n","4. Menampilkan Data yang Telah Didiskritisasi\n","\n","    Dengan menggunakan fungsi tabulate, hasil 10 baris pertama ditampilkan dalam bentuk tabel ASCII yang rapi.\n","\n","Output\n","\n","Hasil akhirnya adalah dataframe baru X_discretized yang berisi label diskrit (seperti 0, 1, 2, atau 3) untuk setiap fitur, tergantung hasil klastering K-Means. Nilai-nilai ini dapat dimodifikasi lebih lanjut (misalnya diubah menjadi huruf: A, B, C, D) untuk interpretasi yang lebih intuitif.\n","\n"],"metadata":{"id":"g2RxWozco-Ed"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","from tabulate import tabulate\n","\n","# Salin data asli\n","X_discretized = X.copy()\n","\n","# Atur jumlah klaster berbeda per fitur\n","cluster_map = {\n","    'sepal length (cm)': 4,\n","    'sepal width (cm)': 3,\n","    'petal length (cm)': 4,\n","    'petal width (cm)': 3\n","}\n","\n","# Lakukan clustering per fitur\n","for col in X.columns:\n","    n_clusters = cluster_map[col]\n","    km = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n","    X_discretized[col] = km.fit_predict(X[[col]])\n","\n","# Tampilkan hasil\n","print(tabulate(X_discretized, headers='keys', tablefmt='psql'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9UAsBHp47if","executionInfo":{"status":"ok","timestamp":1749808486978,"user_tz":-420,"elapsed":147,"user":{"displayName":"23-112 Fariel Nur Rizky Maulana","userId":"07674155344665806922"}},"outputId":"fc21d48d-959c-4796-e804-7220a2feae1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---------------------+--------------------+---------------------+--------------------+\n","|     |   sepal length (cm) |   sepal width (cm) |   petal length (cm) |   petal width (cm) |\n","|-----+---------------------+--------------------+---------------------+--------------------|\n","|   0 |                   2 |                  0 |                   1 |                  1 |\n","|   1 |                   2 |                  2 |                   1 |                  1 |\n","|   2 |                   2 |                  2 |                   1 |                  1 |\n","|   3 |                   2 |                  2 |                   1 |                  1 |\n","|   4 |                   2 |                  0 |                   1 |                  1 |\n","|   5 |                   1 |                  0 |                   1 |                  1 |\n","|   6 |                   2 |                  0 |                   1 |                  1 |\n","|   7 |                   2 |                  0 |                   1 |                  1 |\n","|   8 |                   2 |                  2 |                   1 |                  1 |\n","|   9 |                   2 |                  2 |                   1 |                  1 |\n","|  10 |                   1 |                  0 |                   1 |                  1 |\n","|  11 |                   2 |                  0 |                   1 |                  1 |\n","|  12 |                   2 |                  2 |                   1 |                  1 |\n","|  13 |                   2 |                  2 |                   1 |                  1 |\n","|  14 |                   1 |                  0 |                   1 |                  1 |\n","|  15 |                   1 |                  0 |                   1 |                  1 |\n","|  16 |                   1 |                  0 |                   1 |                  1 |\n","|  17 |                   2 |                  0 |                   1 |                  1 |\n","|  18 |                   1 |                  0 |                   1 |                  1 |\n","|  19 |                   2 |                  0 |                   1 |                  1 |\n","|  20 |                   1 |                  0 |                   1 |                  1 |\n","|  21 |                   2 |                  0 |                   1 |                  1 |\n","|  22 |                   2 |                  0 |                   1 |                  1 |\n","|  23 |                   2 |                  0 |                   1 |                  1 |\n","|  24 |                   2 |                  0 |                   1 |                  1 |\n","|  25 |                   2 |                  2 |                   1 |                  1 |\n","|  26 |                   2 |                  0 |                   1 |                  1 |\n","|  27 |                   2 |                  0 |                   1 |                  1 |\n","|  28 |                   2 |                  0 |                   1 |                  1 |\n","|  29 |                   2 |                  2 |                   1 |                  1 |\n","|  30 |                   2 |                  2 |                   1 |                  1 |\n","|  31 |                   1 |                  0 |                   1 |                  1 |\n","|  32 |                   2 |                  0 |                   1 |                  1 |\n","|  33 |                   1 |                  0 |                   1 |                  1 |\n","|  34 |                   2 |                  2 |                   1 |                  1 |\n","|  35 |                   2 |                  2 |                   1 |                  1 |\n","|  36 |                   1 |                  0 |                   1 |                  1 |\n","|  37 |                   2 |                  0 |                   1 |                  1 |\n","|  38 |                   2 |                  2 |                   1 |                  1 |\n","|  39 |                   2 |                  0 |                   1 |                  1 |\n","|  40 |                   2 |                  0 |                   1 |                  1 |\n","|  41 |                   2 |                  1 |                   1 |                  1 |\n","|  42 |                   2 |                  2 |                   1 |                  1 |\n","|  43 |                   2 |                  0 |                   1 |                  1 |\n","|  44 |                   2 |                  0 |                   1 |                  1 |\n","|  45 |                   2 |                  2 |                   1 |                  1 |\n","|  46 |                   2 |                  0 |                   1 |                  1 |\n","|  47 |                   2 |                  2 |                   1 |                  1 |\n","|  48 |                   1 |                  0 |                   1 |                  1 |\n","|  49 |                   2 |                  0 |                   1 |                  1 |\n","|  50 |                   3 |                  2 |                   0 |                  2 |\n","|  51 |                   0 |                  2 |                   0 |                  2 |\n","|  52 |                   3 |                  2 |                   0 |                  2 |\n","|  53 |                   1 |                  1 |                   2 |                  2 |\n","|  54 |                   0 |                  2 |                   0 |                  2 |\n","|  55 |                   1 |                  2 |                   0 |                  2 |\n","|  56 |                   0 |                  0 |                   0 |                  2 |\n","|  57 |                   2 |                  1 |                   2 |                  2 |\n","|  58 |                   0 |                  2 |                   0 |                  2 |\n","|  59 |                   2 |                  1 |                   2 |                  2 |\n","|  60 |                   2 |                  1 |                   2 |                  2 |\n","|  61 |                   1 |                  2 |                   2 |                  2 |\n","|  62 |                   0 |                  1 |                   2 |                  2 |\n","|  63 |                   0 |                  2 |                   0 |                  2 |\n","|  64 |                   1 |                  2 |                   2 |                  2 |\n","|  65 |                   0 |                  2 |                   0 |                  2 |\n","|  66 |                   1 |                  2 |                   0 |                  2 |\n","|  67 |                   1 |                  1 |                   2 |                  2 |\n","|  68 |                   0 |                  1 |                   0 |                  2 |\n","|  69 |                   1 |                  1 |                   2 |                  2 |\n","|  70 |                   1 |                  2 |                   0 |                  0 |\n","|  71 |                   0 |                  2 |                   2 |                  2 |\n","|  72 |                   0 |                  1 |                   0 |                  2 |\n","|  73 |                   0 |                  2 |                   0 |                  2 |\n","|  74 |                   0 |                  2 |                   2 |                  2 |\n","|  75 |                   0 |                  2 |                   0 |                  2 |\n","|  76 |                   3 |                  2 |                   0 |                  2 |\n","|  77 |                   0 |                  2 |                   0 |                  0 |\n","|  78 |                   0 |                  2 |                   0 |                  2 |\n","|  79 |                   1 |                  1 |                   2 |                  2 |\n","|  80 |                   1 |                  1 |                   2 |                  2 |\n","|  81 |                   1 |                  1 |                   2 |                  2 |\n","|  82 |                   1 |                  1 |                   2 |                  2 |\n","|  83 |                   0 |                  1 |                   0 |                  2 |\n","|  84 |                   1 |                  2 |                   0 |                  2 |\n","|  85 |                   0 |                  0 |                   0 |                  2 |\n","|  86 |                   0 |                  2 |                   0 |                  2 |\n","|  87 |                   0 |                  1 |                   0 |                  2 |\n","|  88 |                   1 |                  2 |                   2 |                  2 |\n","|  89 |                   1 |                  1 |                   2 |                  2 |\n","|  90 |                   1 |                  1 |                   0 |                  2 |\n","|  91 |                   0 |                  2 |                   0 |                  2 |\n","|  92 |                   1 |                  1 |                   2 |                  2 |\n","|  93 |                   2 |                  1 |                   2 |                  2 |\n","|  94 |                   1 |                  1 |                   2 |                  2 |\n","|  95 |                   1 |                  2 |                   2 |                  2 |\n","|  96 |                   1 |                  2 |                   2 |                  2 |\n","|  97 |                   0 |                  2 |                   2 |                  2 |\n","|  98 |                   2 |                  1 |                   2 |                  2 |\n","|  99 |                   1 |                  2 |                   2 |                  2 |\n","| 100 |                   0 |                  0 |                   3 |                  0 |\n","| 101 |                   1 |                  1 |                   0 |                  0 |\n","| 102 |                   3 |                  2 |                   3 |                  0 |\n","| 103 |                   0 |                  2 |                   3 |                  0 |\n","| 104 |                   0 |                  2 |                   3 |                  0 |\n","| 105 |                   3 |                  2 |                   3 |                  0 |\n","| 106 |                   2 |                  1 |                   0 |                  0 |\n","| 107 |                   3 |                  2 |                   3 |                  0 |\n","| 108 |                   0 |                  1 |                   3 |                  0 |\n","| 109 |                   3 |                  0 |                   3 |                  0 |\n","| 110 |                   0 |                  2 |                   0 |                  0 |\n","| 111 |                   0 |                  1 |                   0 |                  0 |\n","| 112 |                   3 |                  2 |                   3 |                  0 |\n","| 113 |                   1 |                  1 |                   0 |                  0 |\n","| 114 |                   1 |                  2 |                   0 |                  0 |\n","| 115 |                   0 |                  2 |                   0 |                  0 |\n","| 116 |                   0 |                  2 |                   3 |                  0 |\n","| 117 |                   3 |                  0 |                   3 |                  0 |\n","| 118 |                   3 |                  1 |                   3 |                  0 |\n","| 119 |                   0 |                  1 |                   0 |                  2 |\n","| 120 |                   3 |                  2 |                   3 |                  0 |\n","| 121 |                   1 |                  2 |                   0 |                  0 |\n","| 122 |                   3 |                  2 |                   3 |                  0 |\n","| 123 |                   0 |                  1 |                   0 |                  0 |\n","| 124 |                   0 |                  0 |                   3 |                  0 |\n","| 125 |                   3 |                  2 |                   3 |                  0 |\n","| 126 |                   0 |                  2 |                   0 |                  0 |\n","| 127 |                   0 |                  2 |                   0 |                  0 |\n","| 128 |                   0 |                  2 |                   3 |                  0 |\n","| 129 |                   3 |                  2 |                   3 |                  2 |\n","| 130 |                   3 |                  2 |                   3 |                  0 |\n","| 131 |                   3 |                  0 |                   3 |                  0 |\n","| 132 |                   0 |                  2 |                   3 |                  0 |\n","| 133 |                   0 |                  2 |                   0 |                  2 |\n","| 134 |                   0 |                  1 |                   3 |                  2 |\n","| 135 |                   3 |                  2 |                   3 |                  0 |\n","| 136 |                   0 |                  0 |                   3 |                  0 |\n","| 137 |                   0 |                  2 |                   3 |                  0 |\n","| 138 |                   0 |                  2 |                   0 |                  0 |\n","| 139 |                   3 |                  2 |                   3 |                  0 |\n","| 140 |                   0 |                  2 |                   3 |                  0 |\n","| 141 |                   3 |                  2 |                   0 |                  0 |\n","| 142 |                   1 |                  1 |                   0 |                  0 |\n","| 143 |                   3 |                  2 |                   3 |                  0 |\n","| 144 |                   0 |                  0 |                   3 |                  0 |\n","| 145 |                   0 |                  2 |                   0 |                  0 |\n","| 146 |                   0 |                  1 |                   0 |                  0 |\n","| 147 |                   0 |                  2 |                   0 |                  0 |\n","| 148 |                   0 |                  0 |                   3 |                  0 |\n","| 149 |                   1 |                  2 |                   0 |                  0 |\n","+-----+---------------------+--------------------+---------------------+--------------------+\n"]}]},{"cell_type":"markdown","source":["# ‚úÇÔ∏è Pembagian Data (Train-Test Split) untuk Data Asli dan Data Diskritisasi\n","Pada bagian ini, dilakukan proses pembagian data menjadi data latih (training) dan data uji (testing) dengan menggunakan fungsi train_test_split dari sklearn.model_selection.\n","\n","1. Pembagian Data Asli\n","2. Pembagian Data Diskritisasi (Binned)\n","\n","Dengan pembagian ini, kita bisa membandingkan performa model klasifikasi pada data asli dan data yang telah melalui proses diskritisasi."],"metadata":{"id":"NNHCrzqMp-fe"}},{"cell_type":"code","source":["# Asli\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Diskritisasi\n","X_disc_train, X_disc_test, _, _ = train_test_split(X_discretized, y, test_size=0.3, random_state=42)\n"],"metadata":{"id":"Dj-NIOA649xp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üìä Perbandingan Klasifikasi Naive Bayes pada Data Asli dan Data yang Didiskritisasi\n","Pada tahap ini, dilakukan evaluasi performa algoritma Naive Bayes terhadap dua jenis data:\n","\n","1. Data asli (kontinu), yang memuat nilai numerik dari fitur-fitur dataset Iris.\n","2. Data hasil diskritisasi, yaitu data yang telah diklasterisasi menggunakan metode K-Means Clustering dan diubah menjadi nilai kategorikal diskrit.\n","\n","Tujuan dari eksperimen ini adalah untuk mengamati bagaimana pengaruh proses diskritisasi terhadap performa model klasifikasi. Naive Bayes, yang dikenal bekerja cukup baik pada data kategorikal, diuji apakah lebih akurat saat diberi input data diskrit dibanding data numerik kontinu.\n","\n","Setelah proses pelatihan, masing-masing model dievaluasi menggunakan confusion matrix serta metrik klasifikasi seperti precision, recall, dan F1-score. Hasil evaluasi ditampilkan dalam bentuk tabel yang memudahkan pembacaan performa setiap kelas (Setosa, Versicolor, Virginica).\n","\n","Terakhir, nilai akurasi keseluruhan dari masing-masing model juga dibandingkan, sehingga dapat disimpulkan apakah proses diskritisasi membawa dampak positif, negatif, atau netral terhadap kinerja Naive Bayes dalam kasus ini."],"metadata":{"id":"yvbiXuKhqPCo"}},{"cell_type":"code","source":["# Naive Bayes pada data asli\n","nb = GaussianNB()\n","nb.fit(X_train, y_train)\n","y_pred_nb = nb.predict(X_test)\n","acc_nb = accuracy_score(y_test, y_pred_nb)\n","\n","# Naive Bayes pada data diskrit\n","nb_disc = GaussianNB()\n","nb_disc.fit(X_disc_train, y_train)\n","y_pred_nb_disc = nb_disc.predict(X_disc_test)\n","acc_nb_disc = accuracy_score(y_test, y_pred_nb_disc)\n","\n","# Visualisasi\n","def print_classification_table(y_true, y_pred, title=\"\"):\n","    report = classification_report(y_true, y_pred, output_dict=True)\n","    table = []\n","\n","    for label, metrics in report.items():\n","        if label not in ['accuracy', 'macro avg', 'weighted avg']:\n","            row = [label,\n","                   f\"{metrics['precision']:.2f}\",\n","                   f\"{metrics['recall']:.2f}\",\n","                   f\"{metrics['f1-score']:.2f}\",\n","                   f\"{metrics['support']}\"]\n","            table.append(row)\n","\n","    headers = [\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n","    print(tabulate(table, headers=headers, tablefmt=\"psql\"))\n","\n","def print_confusion_matrix(y_true, y_pred, model_name=\"Model\", labels=None):\n","    cm = confusion_matrix(y_true, y_pred)\n","    if labels is None:\n","        labels = [str(i) for i in range(len(cm))]\n","\n","    headers = [\"Actual \\\\ Pred\"] + list(labels)\n","    rows = []\n","    for i, row in enumerate(cm):\n","        rows.append([labels[i]] + list(row))\n","\n","    print(f\"\\nüìã {model_name}\")\n","    print(tabulate(rows, headers=headers, tablefmt=\"psql\"))\n","\n","label_names = iris.target_names.tolist()  # ['setosa', 'versicolor', 'virginica']\n","\n","# Naive Bayes\n","print_confusion_matrix(y_test, y_pred_nb, model_name=\"Naive Bayes - Data Asli\", labels=label_names)\n","print_classification_table(y_test, y_pred_nb)\n","\n","print_confusion_matrix(y_test, y_pred_nb_disc, model_name=\"Naive Bayes - Data Diskrit\", labels=label_names)\n","print_classification_table(y_test, y_pred_nb_disc, title=\"Naive Bayes - Data Diskrit\")\n","\n","print(\"\\n Akurasi (Data Asli):\", acc_nb)\n","print(\" Akurasi (Data Diskritisasi):\", acc_nb_disc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hspO1d0z4_Ui","executionInfo":{"status":"ok","timestamp":1749808487031,"user_tz":-420,"elapsed":8,"user":{"displayName":"23-112 Fariel Nur Rizky Maulana","userId":"07674155344665806922"}},"outputId":"92183f1b-37fa-479c-a47c-2a203396ab57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìã Naive Bayes - Data Asli\n","+-----------------+----------+--------------+-------------+\n","| Actual \\ Pred   |   setosa |   versicolor |   virginica |\n","|-----------------+----------+--------------+-------------|\n","| setosa          |       19 |            0 |           0 |\n","| versicolor      |        0 |           12 |           1 |\n","| virginica       |        0 |            0 |          13 |\n","+-----------------+----------+--------------+-------------+\n","+---------+-------------+----------+------------+-----------+\n","|   Class |   Precision |   Recall |   F1-Score |   Support |\n","|---------+-------------+----------+------------+-----------|\n","|       0 |        1    |     1    |       1    |        19 |\n","|       1 |        1    |     0.92 |       0.96 |        13 |\n","|       2 |        0.93 |     1    |       0.96 |        13 |\n","+---------+-------------+----------+------------+-----------+\n","\n","üìã Naive Bayes - Data Diskrit\n","+-----------------+----------+--------------+-------------+\n","| Actual \\ Pred   |   setosa |   versicolor |   virginica |\n","|-----------------+----------+--------------+-------------|\n","| setosa          |       19 |            0 |           0 |\n","| versicolor      |        0 |           13 |           0 |\n","| virginica       |        0 |            0 |          13 |\n","+-----------------+----------+--------------+-------------+\n","+---------+-------------+----------+------------+-----------+\n","|   Class |   Precision |   Recall |   F1-Score |   Support |\n","|---------+-------------+----------+------------+-----------|\n","|       0 |           1 |        1 |          1 |        19 |\n","|       1 |           1 |        1 |          1 |        13 |\n","|       2 |           1 |        1 |          1 |        13 |\n","+---------+-------------+----------+------------+-----------+\n","\n"," Akurasi (Data Asli): 0.9777777777777777\n"," Akurasi (Data Diskritisasi): 1.0\n"]}]},{"cell_type":"markdown","source":["### ‚úÖ Hasil Evaluasi Naive Bayes: Data Asli vs Data Diskritisasi\n","Setelah model Naive Bayes dilatih dan diuji pada dua versi data, diperoleh hasil sebagai berikut:\n","\n","#### üìå Data Asli (Numerik)\n","Model berhasil mengklasifikasikan sebagian besar data dengan akurasi 97.78%.\n","\n","Terdapat sedikit kesalahan pada kelas versicolor, di mana 1 sampel diklasifikasikan sebagai virginica.\n","\n","Precision, recall, dan F1-score sangat tinggi pada ketiga kelas, menunjukkan bahwa model bekerja sangat baik bahkan pada data numerik.\n","\n","#### üìå Data Diskritisasi (Hasil K-Means)\n","Setelah fitur didiskritisasi menggunakan K-Means Clustering (dengan jumlah klaster berbeda per fitur), model justru menunjukkan performa yang sempurna (akurasi 100%).\n","\n","Semua sampel dari ketiga kelas diklasifikasikan dengan benar, tanpa kesalahan satu pun.\n","\n","Nilai precision, recall, dan F1-score untuk semua kelas adalah 1.00, menunjukkan klasifikasi yang ideal.\n","\n","#### üéØ Kesimpulan\n","Diskritisasi dengan K-Means Clustering justru meningkatkan performa model Naive Bayes, kemungkinan karena:\n","\n","Model Naive Bayes sangat cocok untuk data kategorikal.\n","\n","Diskritisasi membantu menyederhanakan kompleksitas variasi numerik.\n","\n","Dengan demikian, diskritisasi berbasis klaster dapat menjadi strategi preprocessing yang efektif untuk meningkatkan performa klasifikasi, terutama saat menggunakan algoritma probabilistik seperti Naive Bayes."],"metadata":{"id":"3gnxACKPrT7V"}},{"cell_type":"markdown","source":["## üìä Perbandingan Klasifikasi Decision Tree pada Data Asli dan Data yang Didiskritisasi\n","Pada tahap ini, dilakukan evaluasi performa algoritma Decision Tree terhadap dua jenis data:\n","\n","1. Data asli (kontinu), yaitu data numerik dari fitur-fitur dataset Iris seperti panjang dan lebar kelopak serta sepal.\n","\n","2. Data hasil diskritisasi, yaitu data yang telah melalui proses pengelompokan (clustering) menggunakan metode K-Means, lalu dikonversi ke dalam bentuk nilai kategorikal diskrit.\n","\n","Tujuan dari eksperimen ini adalah untuk mengamati dampak dari proses diskritisasi terhadap performa model klasifikasi Decision Tree. Sebagai algoritma berbasis pemisahan atribut secara rekursif, Decision Tree pada dasarnya mampu menangani baik data numerik maupun kategorikal, namun performanya bisa berbeda tergantung struktur data.\n","\n","Setelah model dilatih pada masing-masing versi data, dilakukan evaluasi menggunakan:\n","\n","- Confusion matrix, untuk melihat distribusi prediksi benar dan salah per kelas (Setosa, Versicolor, Virginica).\n","\n","- Metrik evaluasi klasifikasi seperti precision, recall, dan F1-score untuk masing-masing kelas.\n","\n","Terakhir, dilakukan perbandingan terhadap akurasi keseluruhan dari kedua model. Hal ini membantu menentukan apakah proses diskritisasi memberikan keuntungan bagi performa Decision Tree dalam tugas klasifikasi pada dataset Iris ini‚Äîapakah membantu mengurangi noise, meningkatkan pemisahan antar kelas, atau sebaliknya, justru menurunkan ketepatan model."],"metadata":{"id":"Ww2x7omzsZ7x"}},{"cell_type":"code","source":["# Decision Tree pada data asli\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train, y_train)\n","y_pred_dt = dt.predict(X_test)\n","acc_dt = accuracy_score(y_test, y_pred_dt)\n","\n","# Decision Tree pada data diskrit\n","dt_disc = DecisionTreeClassifier(random_state=42)\n","dt_disc.fit(X_disc_train, y_train)\n","y_pred_dt_disc = dt_disc.predict(X_disc_test)\n","acc_dt_disc = accuracy_score(y_test, y_pred_dt_disc)\n","\n","# Visualisasi\n","def print_confusion_matrix(y_true, y_pred, model_name=\"Model\", labels=None):\n","    cm = confusion_matrix(y_true, y_pred)\n","    if labels is None:\n","        labels = [str(i) for i in range(len(cm))]\n","    headers = [\"Actual \\\\ Pred\"] + list(labels)\n","    rows = [[labels[i]] + list(row) for i, row in enumerate(cm)]\n","    print(f\"\\nüìã {model_name}\")\n","    print(tabulate(rows, headers=headers, tablefmt=\"psql\"))\n","\n","def print_classification_report(y_true, y_pred, title=\"\"):\n","    report = classification_report(y_true, y_pred, output_dict=True)\n","    table = []\n","\n","    for label, metrics in report.items():\n","        if label not in ['accuracy', 'macro avg', 'weighted avg']:\n","            row = [label,\n","                   f\"{metrics['precision']:.2f}\",\n","                   f\"{metrics['recall']:.2f}\",\n","                   f\"{metrics['f1-score']:.2f}\",\n","                   f\"{metrics['support']}\"]\n","            table.append(row)\n","\n","    headers = [\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n","    print(tabulate(table, headers=headers, tablefmt=\"psql\"))\n","\n","# Cetak hasil\n","label_names = iris.target_names.tolist()\n","\n","print_confusion_matrix(y_test, y_pred_dt, model_name=\"Decision Tree - Data Asli\", labels=label_names)\n","print_classification_report(y_test, y_pred_dt)\n","\n","print_confusion_matrix(y_test, y_pred_dt_disc, model_name=\"Decision Tree - Data Diskrit\", labels=label_names)\n","print_classification_report(y_test, y_pred_dt_disc)\n","\n","print(\"\\nüå≥ Decision Tree Accuracy\")\n","print(f\"Akurasi (Data Asli): {acc_dt:.2f}\")\n","print(f\"Akurasi (Data Diskritisasi): {acc_dt_disc:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8w42m0W65H-m","executionInfo":{"status":"ok","timestamp":1749808487064,"user_tz":-420,"elapsed":22,"user":{"displayName":"23-112 Fariel Nur Rizky Maulana","userId":"07674155344665806922"}},"outputId":"e410837d-7321-47cc-f352-97bc81e39ec3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìã Decision Tree - Data Asli\n","+-----------------+----------+--------------+-------------+\n","| Actual \\ Pred   |   setosa |   versicolor |   virginica |\n","|-----------------+----------+--------------+-------------|\n","| setosa          |       19 |            0 |           0 |\n","| versicolor      |        0 |           13 |           0 |\n","| virginica       |        0 |            0 |          13 |\n","+-----------------+----------+--------------+-------------+\n","+---------+-------------+----------+------------+-----------+\n","|   Class |   Precision |   Recall |   F1-Score |   Support |\n","|---------+-------------+----------+------------+-----------|\n","|       0 |           1 |        1 |          1 |        19 |\n","|       1 |           1 |        1 |          1 |        13 |\n","|       2 |           1 |        1 |          1 |        13 |\n","+---------+-------------+----------+------------+-----------+\n","\n","üìã Decision Tree - Data Diskrit\n","+-----------------+----------+--------------+-------------+\n","| Actual \\ Pred   |   setosa |   versicolor |   virginica |\n","|-----------------+----------+--------------+-------------|\n","| setosa          |       19 |            0 |           0 |\n","| versicolor      |        0 |           13 |           0 |\n","| virginica       |        0 |            0 |          13 |\n","+-----------------+----------+--------------+-------------+\n","+---------+-------------+----------+------------+-----------+\n","|   Class |   Precision |   Recall |   F1-Score |   Support |\n","|---------+-------------+----------+------------+-----------|\n","|       0 |           1 |        1 |          1 |        19 |\n","|       1 |           1 |        1 |          1 |        13 |\n","|       2 |           1 |        1 |          1 |        13 |\n","+---------+-------------+----------+------------+-----------+\n","\n","üå≥ Decision Tree Accuracy\n","Akurasi (Data Asli): 1.00\n","Akurasi (Data Diskritisasi): 1.00\n"]}]},{"cell_type":"markdown","source":["### üå≥ Evaluasi Model Decision Tree: Data Asli vs Data Diskritisasi\n","Pada bagian ini, digunakan algoritma Decision Tree untuk mengklasifikasikan dataset Iris, baik dalam bentuk asli (numerik) maupun setelah didiskritisasi menggunakan metode K-Means Clustering.\n","\n","#### üìå Data Asli (Numerik)\n","Model Decision Tree pertama dilatih menggunakan data numerik hasil train-test split.\n","\n","Setelah dilakukan prediksi terhadap data uji, dilakukan evaluasi melalui dua metode:\n","\n","- Confusion Matrix, yang menunjukkan distribusi prediksi benar dan salah untuk masing-masing kelas (Setosa, Versicolor, Virginica).\n","\n","- Classification Report, yang menyajikan metrik evaluasi utama seperti:\n","\n","    -  Precision (ketepatan prediksi)\n","\n","    - Recall (kemampuan mendeteksi kelas sebenarnya)\n","\n","    - F1-Score (harmoni antara precision dan recall)\n","\n","    - Support (jumlah data di tiap kelas)\n","\n","Hasil evaluasi menunjukkan bahwa model berhasil mengklasifikasikan semua sampel dengan benar, menghasilkan akurasi 100% dan nilai sempurna pada semua metrik evaluasi.\n","\n","#### üìå Data Diskritisasi (K-Means)\n","Selanjutnya, model Decision Tree kedua dilatih pada data yang telah didiskritisasi. Setiap fitur numerik dikonversi menjadi kategori diskrit menggunakan K-Means Clustering.\n","\n","- Proses evaluasi dilakukan dengan cara yang sama seperti sebelumnya:\n","\n","- Prediksi terhadap data uji\n","\n","- Pencetakan confusion matrix\n","\n","- Penyusunan classification report\n","\n","Hasil evaluasi menunjukkan bahwa model juga mencapai performa sempurna ‚Äî akurasi 100%, dengan semua metrik (precision, recall, f1-score) bernilai 1.00 untuk setiap kelas.\n","\n","#### ‚úÖ Kesimpulan\n","Baik pada data numerik maupun diskrit, algoritma Decision Tree menunjukkan performa yang sangat baik dan konsisten. Ini mengindikasikan bahwa:\n","\n","Decision Tree fleksibel terhadap jenis input, baik numerik maupun kategorikal.\n","\n","Proses diskritisasi tidak menurunkan kinerja model, bahkan bisa menjadi alternatif yang valid ketika fitur harus dikonversi.\n","\n","\n","### üìä Perbandingan Global Model Klasifikasi: Naive Bayes vs Decision Tree\n","Setelah dilakukan pengujian pada dua algoritma klasifikasi ‚Äî Naive Bayes dan Decision Tree ‚Äî pada dua jenis data (asli dan hasil diskritisasi), berikut ringkasan dan perbandingannya:\n","\n","- Model\tTipe Data\tAkurasi\tCatatan Performa\n","Naive Bayes\tAsli (numerik)\t0.89\tBeberapa kesalahan klasifikasi, terutama antar kelas yang mirip seperti Versicolor vs Virginica.\n","Naive Bayes\tDiskrit\t1.00\tPerforma meningkat setelah diskritisasi, cocok untuk data kategorikal.\n","- Decision Tree\tAsli (numerik)\t1.00\tKlasifikasi sempurna, model sangat cocok untuk data numerik dengan batas kelas yang jelas.\n","Decision Tree\tDiskrit\t1.00\tPerforma tetap optimal meski fitur telah didiskritisasi.\n","\n","### üîç Insight Utama\n","Diskritisasi meningkatkan performa Naive Bayes, sesuai ekspektasi, karena algoritma ini memang dirancang untuk bekerja lebih optimal pada fitur kategorikal.\n","\n","Decision Tree sangat kuat, tidak terpengaruh secara negatif oleh proses diskritisasi. Ini menandakan fleksibilitas tinggi dari model terhadap berbagai jenis data.\n","\n","Decision Tree unggul dalam hal akurasi, tetapi untuk situasi tertentu (misal data besar atau kebutuhan interpretabilitas tinggi), Naive Bayes tetap relevan karena lebih cepat dan sederhana.\n","\n"],"metadata":{"id":"87FZQO2EtO9z"}}]}